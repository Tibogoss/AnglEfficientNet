{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fm4DtI6cOjNk"
   },
   "source": [
    "# EfficientNetV2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "DCIqam6XOldY"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-13 16:57:49.810636: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-12-13 16:57:49.913803: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-12-13 16:57:50.520477: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-12-13 16:57:50.520528: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-12-13 16:57:50.520534: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from keras.preprocessing.image import ImageDataGenerator \n",
    "from keras.models import Sequential, Model \n",
    "from keras.layers import Conv2D, MaxPooling2D, AveragePooling2D, MaxPooling1D, GlobalAveragePooling2D \n",
    "from keras.layers import Activation, Dropout, Flatten, Dense, Input \n",
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler, ReduceLROnPlateau \n",
    "from keras.callbacks import TensorBoard\n",
    "from keras import backend as K \n",
    "from keras.optimizers import Adam, SGD, Adadelta\n",
    "from keras.regularizers import l2, l1\n",
    "\n",
    "from keras.callbacks import CSVLogger\n",
    "import sys\n",
    "import time\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from keras.utils.vis_utils import plot_model\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "hPkg44AWO8rK"
   },
   "outputs": [],
   "source": [
    "# Loading images from Google Drive\n",
    "train_data_dir = 'images/train'\n",
    "validation_data_dir = 'images/test'\n",
    "nb_train_samples = 67988 \n",
    "nb_validation_samples = 22716\n",
    "n_classes = 101\n",
    "epochs = 10\n",
    "batch_size = 75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "FZZJ_2kSPCzl"
   },
   "outputs": [],
   "source": [
    "# Checking image format: if RGB channel is coming first or last so, model will check first and then input shape will be feeded accordingly.\n",
    "img_width = 299\n",
    "img_height = 299\n",
    "\n",
    "if K.image_data_format() == 'channels_first': \n",
    "    input_shape = (3, img_width, img_height) \n",
    "else: \n",
    "    input_shape = (img_width, img_height, 3) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "executionInfo": {
     "elapsed": 17747,
     "status": "ok",
     "timestamp": 1596262929445,
     "user": {
      "displayName": "Gianmarco Ria",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjMp7KZ0us4lc7vwWhGAGg6qMuFNg8TVQSFAKb9GA=s64",
      "userId": "01406154558516398149"
     },
     "user_tz": -120
    },
    "id": "81jIYoEbPOOt",
    "outputId": "97bf21b2-f5c3-4327-84fb-ea836862e019"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 67988 images belonging to 101 classes.\n",
      "Found 22716 images belonging to 101 classes.\n"
     ]
    }
   ],
   "source": [
    "# Train & Test Data Generators with image augmentation \n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "    width_shift_range=0.2,  # randomly shift images horizontally (fraction of total width)\n",
    "    height_shift_range=0.2,  # randomly shift images vertically (fraction of total height)\n",
    "    horizontal_flip=True,  # randomly flip images\n",
    "    zoom_range=[.8, 1],\n",
    "    channel_shift_range=30,\n",
    "    fill_mode='reflect')\n",
    "\n",
    "test_datagen = ImageDataGenerator()\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_data_dir,\n",
    "    target_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    seed = 11,\n",
    "    class_mode='categorical')\n",
    "\n",
    "validation_generator = test_datagen.flow_from_directory(\n",
    "    validation_data_dir,\n",
    "    target_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    seed = 11,\n",
    "    class_mode='categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications.efficientnet_v2 import EfficientNetV2S\n",
    "base_model = EfficientNetV2S(weights='imagenet', include_top=False, input_tensor=Input(shape=(299, 299, 3)))\n",
    "x = base_model.output\n",
    "x = AveragePooling2D(pool_size=(8, 8))(x)\n",
    "x = Dropout(.4)(x)\n",
    "x = Flatten()(x)\n",
    "\n",
    "predictions = Dense(n_classes,\n",
    "                    kernel_regularizer=l2(0.005),\n",
    "                    activity_regularizer=l1(0.005), \n",
    "                    activation='softmax')(x)\n",
    "\n",
    "model = Model(base_model.input, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "1zNnAXKOPMce"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ktg3317/.local/lib/python3.10/site-packages/keras/optimizers/optimizer_v2/gradient_descent.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "model.compile(\n",
    "    optimizer=SGD(lr=.01, momentum=.9), \n",
    "    loss='categorical_crossentropy', \n",
    "    metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "6sGjuJtKPQHH"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras import callbacks\n",
    "cp = callbacks.ModelCheckpoint(checkpoint_path, monitor='val_accuracy',save_best_only=True,verbose=1, mode='max')\n",
    "csv_logger = callbacks.CSVLogger('efficientv2/InceptionV3.log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "SPmLGkhHIX8j"
   },
   "outputs": [],
   "source": [
    "reduce_lr = ReduceLROnPlateau(monitor='val_accuracy', factor=0.1,\n",
    "                              patience=2, min_lr=0.00001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7fzxKXNtmFfl"
   },
   "source": [
    "**Model Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 887
    },
    "executionInfo": {
     "elapsed": 24001895,
     "status": "error",
     "timestamp": 1592760826806,
     "user": {
      "displayName": "Gianmarco Ria",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjMp7KZ0us4lc7vwWhGAGg6qMuFNg8TVQSFAKb9GA=s64",
      "userId": "01406154558516398149"
     },
     "user_tz": -120
    },
    "id": "cgo3IRSePaW6",
    "outputId": "f698c415-6b22-46bd-b929-a0735425f199"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-12 08:03:40.465138: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:954] layout failed: INVALID_ARGUMENT: Size of values 0 does not match size of permutation 4 @ fanin shape inmodel/block1b_drop/dropout/SelectV2-2-TransposeNHWCToNCHW-LayoutOptimizer\n",
      "2023-12-12 08:03:51.273924: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:428] Loaded cuDNN version 8903\n",
      "2023-12-12 08:03:58.955502: I tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:630] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "906/906 [==============================] - ETA: 0s - loss: 3.0451 - accuracy: 0.4354"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-12 08:32:47.005086: W tensorflow/tsl/framework/bfc_allocator.cc:290] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.94GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2023-12-12 08:32:47.005131: W tensorflow/tsl/framework/bfc_allocator.cc:290] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.94GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "906/906 [==============================] - 2022s 2s/step - loss: 3.0451 - accuracy: 0.4354 - val_loss: 1.9191 - val_accuracy: 0.6288 - lr: 0.0100\n",
      "Epoch 2/10\n",
      "906/906 [==============================] - 1303s 1s/step - loss: 1.8279 - accuracy: 0.6323 - val_loss: 1.6462 - val_accuracy: 0.6648 - lr: 0.0100\n",
      "Epoch 3/10\n",
      "906/906 [==============================] - 1302s 1s/step - loss: 1.5460 - accuracy: 0.6834 - val_loss: 1.5528 - val_accuracy: 0.6839 - lr: 0.0100\n",
      "Epoch 4/10\n",
      "906/906 [==============================] - 1305s 1s/step - loss: 1.3687 - accuracy: 0.7207 - val_loss: 1.4811 - val_accuracy: 0.7008 - lr: 0.0100\n",
      "Epoch 5/10\n",
      "906/906 [==============================] - 1274s 1s/step - loss: 1.2320 - accuracy: 0.7510 - val_loss: 1.4846 - val_accuracy: 0.7035 - lr: 0.0100\n",
      "Epoch 6/10\n",
      "906/906 [==============================] - 1273s 1s/step - loss: 1.1133 - accuracy: 0.7758 - val_loss: 1.4553 - val_accuracy: 0.7156 - lr: 0.0100\n",
      "Epoch 7/10\n",
      "906/906 [==============================] - 1242s 1s/step - loss: 1.0112 - accuracy: 0.7986 - val_loss: 1.4784 - val_accuracy: 0.7123 - lr: 0.0100\n",
      "Epoch 8/10\n",
      "906/906 [==============================] - 1230s 1s/step - loss: 0.9243 - accuracy: 0.8188 - val_loss: 1.4592 - val_accuracy: 0.7227 - lr: 0.0100\n",
      "Epoch 9/10\n",
      "906/906 [==============================] - 1230s 1s/step - loss: 0.8408 - accuracy: 0.8371 - val_loss: 1.4941 - val_accuracy: 0.7194 - lr: 0.0100\n",
      "Epoch 10/10\n",
      "906/906 [==============================] - 1224s 1s/step - loss: 0.7826 - accuracy: 0.8492 - val_loss: 1.5230 - val_accuracy: 0.7197 - lr: 0.0100\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7ff8b7f21e10>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_generator,\n",
    "          steps_per_epoch = nb_train_samples // batch_size,\n",
    "          validation_data=validation_generator,\n",
    "          validation_steps=nb_validation_samples // batch_size,\n",
    "          epochs=10,\n",
    "          verbose=1,\n",
    "          callbacks=[csv_logger, reduce_lr]\n",
    "          )"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyP4eKJrxYaifoITqELP3kXQ",
   "collapsed_sections": [],
   "mount_file_id": "1X14xCfzx6ZvMWReOcYpY5p8KO-6AZ0Zo",
   "name": "InceptionV3_Food101.ipynb",
   "provenance": [
    {
     "file_id": "1X14xCfzx6ZvMWReOcYpY5p8KO-6AZ0Zo",
     "timestamp": 1601221065539
    },
    {
     "file_id": "1_jELBtYtC-W7DlTzIkq2j1ImMLqo1A5O",
     "timestamp": 1587216518300
    },
    {
     "file_id": "1NPFL0BQYBJg87UmqtnTiao9hwsWgNiDq",
     "timestamp": 1586868903489
    },
    {
     "file_id": "1MWVob8RC-7VCnDBTOXBGCILA6zi0vTXW",
     "timestamp": 1586530655844
    },
    {
     "file_id": "1NPh3PL3erXpaMYv8u8xviCw69wxsPLxJ",
     "timestamp": 1586259261320
    }
   ]
  },
  "kernelspec": {
   "display_name": "tensor",
   "language": "python",
   "name": "tensor"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
